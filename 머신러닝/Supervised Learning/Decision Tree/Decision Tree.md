### Decision Tree

결정 트리(decision tree)는 데이터를 분류하거나 결과값을 예측하는 분석 방법이다. 결과 모델이 트리 구조이기 때문에 결정 트리라고 한다. 결정 트리는 이상치가 많은 값으로 구성된 데이터셋을 다룰 때 사용하면 좋다. 또한, 결정 과정이 시각적으로 표현되기 때문에 머신 러닝이 어떤 방식으로 의사 결정을 하는지 알고 싶을 때 유용하다. 아래 그림은 결정 과정을 보여준다.

![tree](https://user-images.githubusercontent.com/79203421/201366880-b3aae42c-e8ba-4698-b290-30cea3a2b994.png)

결정 트리는 데이터를 1차로 분류한 후 각 영역의 순도(homogeneity)는 증가하고, 불순도(impurity)와 불확실성(uncertainty)는 감소하는 방향으로 학습을 진행한다. 순도가 증가하고 불확실성이 감소하는 것을 정보 이론에서는 정보 획득(Information gain)이라고 하며, 순도를 계산하는 방법에는 다음 두 가지를 많이 사용한다.

💡 **순도와 불순도** 

순도는 범주 안에서 같은 종류의 데이터만 모여 있는 상태이며, 불순도는 서로 다른 데이터가 섞여 있는 상태이다.


**1. 엔트로피(entropy)**
    
    확률 변수의 불확실성을 수치로 나타낸 것으로, 엔트로피가 높을수록 불확실성이 높다는 의미이다. 즉, 엔트로피 값이 0과 0.5라고 가정하면 다음의 도출이 가능하다.
    
    - 엔트로피 = 0 = 불확실성 최소 = 순도 최대
    - 엔트로피 = 0.5 = 불확실성 최대 = 순도 최소
    
    레코드 m개가 A 영역에 포함되어 있다면 엔트로피는 다음 식으로 정의된다. 아래 수식에서 $p_k$는 $A$ 영역에 속하는 데이터 가운데 k 범주에 속하는 데이터의 비율이다.
    
    - $Entropy(A) =- \sum_{k=1}^mp_klog_2(p_k)$
    
       
    
**2. 지니 계수(Gini index)**
    
    불순도를 측정하는 지표로, 데이터의 통계적 분산 정도를 정량화해서 표현한 값이다. 즉, 지니 계수는 원소 n개 중에서 임의로 두 개를 추출햇을 때, 추출된 두 개가 서로 다른 그룹에 속해있을 확률을 의미한다. 지니 계수는 다음 공식으로 구할 수 있고, S는 이미 발생한 사건의 모음, c는 사건 개수이다. 지니 계수가 높을수록 데이터가 분산되어 있음을 의미한다. 
    
    - $G(S) =1-\sum_{i=1}^cp_i^2$
    
    지니 계수는 로그를 계산할 필요가 없어 엔트로피보다 계산이 빠르기 때문에 결정 트리에서 많이 사용한다.
